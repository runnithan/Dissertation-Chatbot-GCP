# app.py

import os
from pathlib import Path
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from dotenv import load_dotenv

from rag_pipeline.query_pipeline import (
    load_embedding_model,
    load_llm,
    load_tokenizer,
    query_rag_pipeline
)
from rag_pipeline.gcs_loader import (
    load_chunks,
    load_embeddings,
    load_faiss_index
)

# --- Load environment ---
load_dotenv()

# --- Diagnostics ---
if os.getenv("K_SERVICE"):
    print("üöÄ Running in Cloud Run. Secrets come from Secret Manager.")
else:
    print("üíª Running locally (.env).")

if os.getenv("GROQ_API_KEY"):
    print("‚úÖ GROQ_API_KEY detected.")
else:
    print("‚ö†Ô∏è GROQ_API_KEY missing!")

# --- App setup ---
app = FastAPI()
ROOT_DIR = Path(__file__).resolve().parent
app.mount("/static", StaticFiles(directory="static"), name="static")

# --- Health check ---
@app.get("/health")
def health_check():
    return {
        "status": "ok",
        "running_in": "cloud" if os.getenv("K_SERVICE") else "local",
        "groq_key_loaded": bool(os.getenv("GROQ_API_KEY"))
    }

# --- Load RAG components eagerly ---
print("üîß Initialising RAG pipeline...")

MODEL_VERSION = os.getenv("MODEL_VERSION", "v1.0")
GCS_BUCKET = os.getenv("GCS_BUCKET", "dissertation-chatbot-data")

print(f"üß† Loading model version: {MODEL_VERSION}")
print(f"üì¶ Using bucket: {GCS_BUCKET}")

try:
    chunks = load_chunks(model_version=MODEL_VERSION)
    embeddings = load_embeddings(model_version=MODEL_VERSION)
    faiss_index = load_faiss_index(model_version=MODEL_VERSION)
    print(f"‚úÖ Loaded {len(chunks)} chunks.")
except Exception as e:
    import traceback
    print("‚ö†Ô∏è Failed to load RAG data from GCS:")
    print(traceback.format_exc())
    chunks = embeddings = faiss_index = None

# Always load these locally so app still starts
embedding_model = load_embedding_model()
llm_pipeline = load_llm(mode="cloud")
tokenizer = load_tokenizer()

print("‚úÖ Components ready (startup non-blocking).")

# --- Serve frontend ---
@app.get("/")
def serve_frontend():
    return FileResponse(ROOT_DIR / "static" / "index.html")

# --- Query endpoint ---
@app.post("/query")
async def handle_query(request: Request):
    body = await request.json()
    question = body.get("question", "").strip()

    if not question:
        return JSONResponse(content={"answer": "‚ö†Ô∏è Please provide a valid question."})

    try:
        answer = query_rag_pipeline(
            question,
            embedding_model,
            faiss_index,
            chunks,
            llm_pipeline,
            tokenizer
        )
        return JSONResponse(content={"answer": answer})
    except Exception as e:
        import traceback
        print("‚ùå Error in query_rag_pipeline:", traceback.format_exc())
        return JSONResponse(
            status_code=500,
            content={"answer": "‚ö†Ô∏è Internal error. Please try again later."}
        )
